{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "secOC4gkSMix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Extract the first 20 tokens from text1\n",
        "\n",
        "The tokens() method returns the document that this concordance index was created froml retun type is list(str).\n",
        "\n",
        "Text objects provide methods performing a variety of analyses on the text's contexts and displaying the results."
      ],
      "metadata": {
        "id": "fkR6vBHMUXGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7hLsOqoSMlK",
        "outputId": "a41ddd33-7681-4967-dffc-aac22b4ba507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.tokens[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvuAFnmLSMnt",
        "outputId": "90578d1d-d9a4-4acf-d6b7-bae09c91ff67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Concordance for text1 word 'sea', selecting only 5 lines"
      ],
      "metadata": {
        "id": "gA2xQbPWXy2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance('sea', lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxS5SZ-7U_OV",
        "outputId": "3a0f2966-cdb0-43f4-c2f8-bac7c8dfed90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Experiment with the nltk count method and Python's count method\n",
        "\n",
        "NLTK's count method returns self.tokens.count(word), so when we call it, we can simply do 'text.count(word)'.\n",
        "\n",
        "While when we use Python's count method, we need to use a list object to call the method, such as 'text.tokens.count(word)'."
      ],
      "metadata": {
        "id": "_F1toCRiU_iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk count()\n",
        "text1.count('the')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IffzQedOSMqG",
        "outputId": "ccb1a238-8df0-4754-f82d-8d8f90aca27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13721"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#python count()\n",
        "text1.tokens.count('the')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXTuGmkUU_RL",
        "outputId": "bc0c7b05-d28e-4a50-91da-01e9aa8534c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13721"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use raw text of at least 5 sentences and save the text into a variable called raw_text\n",
        "```\n",
        "raw_text= \n",
        "'What doesn't kill you makes you stronger. \n",
        "Stand a little taller. \n",
        "Doesn't mean I'm lonely when I'm alone. \n",
        "What doesn't kill you makes a fighter. \n",
        "Footsteps even lighter. \n",
        "Doesn't mean I'm over 'cause you're gone'\n",
        "```\n",
        "    source: https://www.azlyrics.com/lyrics/kellyclarkson/whatdoesntkillyoustronger.html\n",
        "\n",
        "\n",
        "\n",
        "* Use NLTK's word tokenizer, tokenize the text into variable 'tokens'\n",
        "* Print the first 10 tokens"
      ],
      "metadata": {
        "id": "3vwDIA5lYe4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text= 'What doesn\\'t kill you makes you stronger. Stand a little taller. Doesn\\'t mean I\\'m lonely when I\\'m alone. What doesn\\'t kill you makes a fighter. Footsteps even lighter. Doesn\\'t mean I\\'m over \\'cause you\\'re gone'\n",
        "\n",
        "from nltk import word_tokenize\n",
        "print(word_tokenize(raw_text)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh1KNSCeU_L_",
        "outputId": "23cf40cb-b53d-450a-ab57-3c2843a3d929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'does', \"n't\", 'kill', 'you', 'makes', 'you', 'stronger', '.', 'Stand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Perform sentence segmentation using NLTK's sentence tokenizer and display the sentences"
      ],
      "metadata": {
        "id": "EEhERlA6amp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "print(sent_tokenize(raw_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6zPdexFbCuG",
        "outputId": "eca95f37-12c9-48c4-ce1e-51aefea6c4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"What doesn't kill you makes you stronger.\", 'Stand a little taller.', \"Doesn't mean I'm lonely when I'm alone.\", \"What doesn't kill you makes a fighter.\", 'Footsteps even lighter.', \"Doesn't mean I'm over 'cause you're gone\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using NLTK's PorterStemmer()\n",
        "* Write a list comprehension to stem the text. Display the list.\n"
      ],
      "metadata": {
        "id": "coRNzcBQbTII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer= PorterStemmer()\n",
        "stemmed= [stemmer.stem(t) for t in word_tokenize(raw_text)]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLFpGFK7U_Jw",
        "outputId": "1dd724aa-9dda-4924-a68e-a55af9b10ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'doe', \"n't\", 'kill', 'you', 'make', 'you', 'stronger', '.', 'stand', 'a', 'littl', 'taller', '.', 'doe', \"n't\", 'mean', 'i', \"'m\", 'lone', 'when', 'i', \"'m\", 'alon', '.', 'what', 'doe', \"n't\", 'kill', 'you', 'make', 'a', 'fighter', '.', 'footstep', 'even', 'lighter', '.', 'doe', \"n't\", 'mean', 'i', \"'m\", 'over', \"'caus\", 'you', \"'re\", 'gone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use NLTK's WordNetLemmatizer\n",
        "* Write a list comprehension to lemmatize the text. Display the list.\n",
        "* Some differences I see in the stems verses the lemmas:\n",
        "  - little - little\n",
        "  - doe - does\n",
        "  - lone - lonely\n",
        "  - alon - alone\n",
        "  - footstep - footsteps\n",
        "  - caus - cause\n",
        "  "
      ],
      "metadata": {
        "id": "ua4xeiSib4JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl= WordNetLemmatizer()\n",
        "lemmatized= [wnl.lemmatize(t) for t in word_tokenize(raw_text)]\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiQRZbeaU_HC",
        "outputId": "65198a0f-6345-4281-96f3-84c04766e2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'doe', \"n't\", 'kill', 'you', 'make', 'you', 'stronger', '.', 'Stand', 'a', 'little', 'taller', '.', 'Does', \"n't\", 'mean', 'I', \"'m\", 'lonely', 'when', 'I', \"'m\", 'alone', '.', 'What', 'doe', \"n't\", 'kill', 'you', 'make', 'a', 'fighter', '.', 'Footsteps', 'even', 'lighter', '.', 'Does', \"n't\", 'mean', 'I', \"'m\", 'over', \"'cause\", 'you', \"'re\", 'gone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The NLTK library is quite capable and offers several text-manipulating functionalities, including stemming, tokenization, filtering stop words, and POS tagging, to name a few. With NLTK, text processing is rather simple. The library's code quality is top notch. Every method has a documentation string, therefore it is well documented. Also, there is relatively little redundant code, which makes it pretty tidy. I'll certainly make use of this powerful library for future projects to carry out text normalization, filter text documents, and use POS tagging to further analyze incoming content.\n",
        "\n"
      ],
      "metadata": {
        "id": "koGls2Nwd29z"
      }
    }
  ]
}